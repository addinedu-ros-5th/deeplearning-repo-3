{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "class VehicleTracker:\n",
    "    def __init__(self):\n",
    "        self.background_subtractor = cv2.createBackgroundSubtractorMOG2()\n",
    "        self.prev_frame = None\n",
    "\n",
    "    def track_vehicle(self, frame):\n",
    "        # 배경 제거 수행\n",
    "        fg_mask = self.background_subtractor.apply(frame)\n",
    "\n",
    "        # 이전 프레임이 없는 경우 현재 프레임을 이전 프레임으로 설정하고 반환\n",
    "        if self.prev_frame is None:\n",
    "            self.prev_frame = frame\n",
    "            return False\n",
    "\n",
    "        # 현재 프레임과 이전 프레임의 차이를 계산\n",
    "        frame_diff = cv2.absdiff(frame, self.prev_frame)\n",
    "        diff_gray = cv2.cvtColor(frame_diff, cv2.COLOR_BGR2GRAY)\n",
    "        _, diff_thresh = cv2.threshold(diff_gray, 30, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # 차이가 있는 영역의 개수를 계산\n",
    "        num_diff_pixels = cv2.countNonZero(diff_thresh)\n",
    "\n",
    "        # 차이가 있는 영역이 일정 개수 이상인 경우 차량이 움직인 것으로 판단\n",
    "        if num_diff_pixels > 5000:  # 적절한 임계값을 설정해야 합니다.\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def main(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    tracker = VehicleTracker()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        \n",
    "        # 차량 추적 및 이동 상태 판단\n",
    "        is_moving = tracker.track_vehicle(frame)\n",
    "        driving_status = \"driving\" if is_moving else \"stop\"\n",
    "        \n",
    "        # 주행 상태를 화면에 표시\n",
    "        cv2.putText(frame, f\"status: {driving_status}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        # 결과 출력\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"/home/min/dev_ws/machine_learning/project_3rd/data/TEST_VIDEO/6_Fail_Pedestrian_Red_Light.MOV\"\n",
    "    main(video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/min/dev_ws/machine_learning/project_3rd/data/TEST_VIDEO/6_Fail_Pedestrian_Red_Light.MOV\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/min/dev_ws/machine_learning/project_3rd/src/model_learning/yolo_detection/all_best.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 60\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(video_path, model_path)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(video_path, model_path):\n\u001b[1;32m     31\u001b[0m     cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(video_path)\n\u001b[0;32m---> 32\u001b[0m     detector \u001b[38;5;241m=\u001b[39m \u001b[43mDetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m     35\u001b[0m         success, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m, in \u001b[0;36mDetector.__init__\u001b[0;34m(self, model_path, confidence_threshold)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path, confidence_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfidence_threshold \u001b[38;5;241m=\u001b[39m confidence_threshold\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     12\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m416\u001b[39m, \u001b[38;5;241m416\u001b[39m)),\n\u001b[1;32m     13\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     14\u001b[0m     ])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "class Detector:\n",
    "    def __init__(self, model_path, confidence_threshold=0.5):\n",
    "        self.model = torch.load(model_path)\n",
    "        self.model.eval()\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((416, 416)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def detect_objects(self, frame):\n",
    "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        image_tensor = self.transform(image).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(image_tensor)\n",
    "        boxes = []\n",
    "        for output in outputs:\n",
    "            for detection in output:\n",
    "                class_id = int(detection[5])\n",
    "                confidence = detection[4]\n",
    "                if confidence > self.confidence_threshold:\n",
    "                    boxes.append(detection.tolist())\n",
    "        return boxes\n",
    "\n",
    "def main(video_path, model_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    detector = Detector(model_path)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        \n",
    "        # 객체 감지 수행\n",
    "        boxes = detector.detect_objects(frame)\n",
    "\n",
    "        # 이동 여부 판단\n",
    "        is_moving = len(boxes) > 0\n",
    "        driving_status = \"주행 중\" if is_moving else \"정지\"\n",
    "        \n",
    "        print(f\"현재 주행 상태: {driving_status}\")\n",
    "        \n",
    "        # 결과 출력\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"/home/min/dev_ws/machine_learning/project_3rd/data/TEST_VIDEO/6_Fail_Pedestrian_Red_Light.MOV\"\n",
    "    model_path = \"/home/min/dev_ws/machine_learning/project_3rd/src/model_learning/yolo_detection/all_best.pt\"\n",
    "    main(video_path, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
